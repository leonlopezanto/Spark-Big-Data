{"cells":[{"cell_type":"markdown","source":["# Actividad 1: HDFS, Spark SQL y MLlib"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c9df0df-6466-442b-be79-a7803a60f44f"}}},{"cell_type":"markdown","source":["## Recuerda borrar siempre las líneas que dicen `raise NotImplementedError`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b3fabad-3e43-4fea-b901-4e326f79af8d"}}},{"cell_type":"markdown","source":["Lee con detenimiento cada ejercicio. Las variables utilizadas para almacenar las soluciones, al igual que las nuevas columnas creadas, deben llamarse **exactamente** como indica el ejercicio, o de lo contrario los tests fallarán y el ejercicio no puntuará. Debe reemplazarse el valor `None` al que están inicializadas por el código necesario para resolver el ejercicio."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f20d20d6-f12f-43c6-bca6-e617d62a538e"}}},{"cell_type":"markdown","source":["## Leemos el fichero flights.csv que hemos subido a HDFS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de6745b3-959a-44f6-be40-a21318dc1a9b"}}},{"cell_type":"markdown","source":["Indicamos que contiene encabezados (nombres de columnas) y que intente inferir el esquema, aunque después comprobaremos si lo\nha inferido correctamente o no. La ruta del archivo en HDFS debería ser /<nombre_alumno>/flights.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe27ed23-19aa-4772-876d-4cb7924efc6d"}}},{"cell_type":"code","source":["ruta_hdfs = '/Antonio_Lopez/flights.csv' # Reemplaza esto por la ruta correcta del fichero flights.csv en HDFS\nflightsDF = None\n\n# Descomentar estas líneas\nflightsDF = spark.read\\\n            .option(\"header\", \"true\")\\\n            .option(\"inferSchema\", \"true\")\\\n            .csv(ruta_hdfs)\n\n# YOUR CODE HERE\nflightsDF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4dee143-ddb6-49d9-a5d8-79726f63ba65"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Imprimimos el esquema para comprobar qué tipo de dato ha inferido en cada columna"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bebabbb9-8c2a-41ba-81e8-f21bd0e3d458"}}},{"cell_type":"code","source":["flightsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ed6d18b-ed38-44ae-8a96-1ce71bfc9d8f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n |-- dep_time: string (nullable = true)\n |-- dep_delay: string (nullable = true)\n |-- arr_time: string (nullable = true)\n |-- arr_delay: string (nullable = true)\n |-- carrier: string (nullable = true)\n |-- tailnum: string (nullable = true)\n |-- flight: integer (nullable = true)\n |-- origin: string (nullable = true)\n |-- dest: string (nullable = true)\n |-- air_time: string (nullable = true)\n |-- distance: integer (nullable = true)\n |-- hour: string (nullable = true)\n |-- minute: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- year: integer (nullable = true)\n-- month: integer (nullable = true)\n-- day: integer (nullable = true)\n-- dep_time: string (nullable = true)\n-- dep_delay: string (nullable = true)\n-- arr_time: string (nullable = true)\n-- arr_delay: string (nullable = true)\n-- carrier: string (nullable = true)\n-- tailnum: string (nullable = true)\n-- flight: integer (nullable = true)\n-- origin: string (nullable = true)\n-- dest: string (nullable = true)\n-- air_time: string (nullable = true)\n-- distance: integer (nullable = true)\n-- hour: string (nullable = true)\n-- minute: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Mostramos el número de filas que tiene el DataFrame para hacernos una idea de su tamaño:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3fb1475-e7f1-48b2-b4f4-7f83ee8134f3"}}},{"cell_type":"code","source":["flightsDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c27d3b4a-2955-42dc-84ff-5f8f04d15b90"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[4]: 162049</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: 162049</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Vemos que tenemos 162049 filas. Si imprimimos por pantalla las 5 primeras filas, veremos qué tipos parecen tener y en qué columnas no coincide el tipo que podríamos esperar con el tipo que ha inferido Spark."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c463612a-84d5-4cfb-b3bf-e1536bb8d6f3"}}},{"cell_type":"code","source":["flightsDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee3703cd-4006-4d62-b629-82003a9b07f6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n|2014|    1|  1|       1|       96|     235|       70|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n|2014|    1|  1|       4|       -6|     738|      -23|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n|2014|    1|  1|       8|       13|     548|       -4|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n|2014|    1|  1|      28|       -2|     800|      -23|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n|2014|    1|  1|      34|       44|     325|       43|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\nyear|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n2014|    1|  1|       1|       96|     235|       70|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n2014|    1|  1|       4|       -6|     738|      -23|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n2014|    1|  1|       8|       13|     548|       -4|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n2014|    1|  1|      28|       -2|     800|      -23|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n2014|    1|  1|      34|       44|     325|       43|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["La causa del problema es que en muchas columnas existe un valor faltante llamado \"NA\". Spark no reconoce ese valor como\n*no disponible* ni nada similar, sino que lo considera como un string de texto normal, y por tanto, asigna a toda la columna\nel tipo de dato string (cadena de caracteres). Concretamente, las siguientes columnas deberían ser de tipo entero pero Spark\nlas muestra como string:\n<ul>\n <li>dep_time: string (nullable = true)\n <li>dep_delay: string (nullable = true)\n <li>arr_time: string (nullable = true)\n <li>arr_delay: string (nullable = true)\n <li>air_time: string (nullable = true)\n <li>hour: string (nullable = true)\n <li>minute: string (nullable = true)    \n</ul>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fe97407-902b-4e62-8d10-3ab9a56ad50e"}}},{"cell_type":"markdown","source":["Vamos a averiguar cuántas filas tienen el valor \"NA\" (como string) en la columna dep_time:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ee19719-7c32-454f-8089-30a9ff531e52"}}},{"cell_type":"code","source":["from pyspark.sql import functions as F\ncuantos_NA = flightsDF\\\n                .where(F.col(\"dep_time\") == \"NA\")\\\n                .count()\ncuantos_NA"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"331ae83d-6b9c-4909-88ea-51753632ad57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[6]: 857</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: 857</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Por tanto, hay 857 filas que no tienen un dato válido en esa columna. Hay distintas maneras de trabajar con los valores faltantes, como por ejemplo imputarlos (reemplazarlos por un valor generado por nosotros según cierta lógica, por ejemplo la media de esa columna, etc). Lo más sencillo es quitar toda la fila, aunque esto depende de si nos lo podemos permitir en base\na la cantidad de datos que tenemos. En nuestro caso, como tenemos un número considerable de filas, vamos a quitar todas las filas donde hay un NA en cualquiera de las columnas."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7cd3b67-869a-46d4-9435-4167c130400b"}}},{"cell_type":"code","source":["columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n\nflightsLimpiado = flightsDF\nfor nombreColumna in columnas_limpiar:  # para cada columna, nos quedamos con las filas que no tienen NA en esa columna\n    flightsLimpiado = flightsLimpiado.where(F.col(nombreColumna) != \"NA\")\n\nflightsLimpiado.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15595b45-e6f1-4529-bcf6-55ff0532d63d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[7]: DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Si ahora mostramos el número de filas que tiene el DataFrame `flightsLimpiado` tras eliminar todas esas filas, vemos que ha disminuido ligeramente\npero sigue siendo un número considerable como para realizar analítica y sacar conclusiones sobre estos datos"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcc9b5de-fe46-4be5-9bdd-b8f1d08c4e99"}}},{"cell_type":"code","source":["flightsLimpiado.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae29ffe7-d410-4901-8c2d-16db943870e7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[8]: 160748</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: 160748</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Una vez que hemos eliminado los NA, vamos a convertir a tipo entero cada una de esas columnas que eran de tipo string. \nAhora no debe haber problema ya que todas las cadenas de texto contienen dentro un número que puede ser convertido de texto a número. Vamos también a convertir la columna `arr_delay` de tipo entero a número real, necesario para los pasos posteriores donde ajustaremos un modelo predictivo."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f66fead2-7594-41da-92ee-b42ed49cd6ef"}}},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType, DoubleType\n\nflightsConvertido = flightsLimpiado\n\nfor c in columnas_limpiar:\n    # método que crea una columna o reemplaza una existente\n    flightsConvertido = flightsConvertido.withColumn(c, F.col(c).cast(IntegerType())) \n\nflightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType()))\nflightsConvertido.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9cc42f7b-12b0-4066-ba8c-b94eb4c906f1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: DataFrame[year: int, month: int, day: int, dep_time: int, dep_delay: int, arr_time: int, arr_delay: double, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: int, distance: int, hour: int, minute: int]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: DataFrame[year: int, month: int, day: int, dep_time: int, dep_delay: int, arr_time: int, arr_delay: double, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: int, distance: int, hour: int, minute: int]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["flightsConvertido.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b45e7b2-d2e5-4966-93b5-09bd31937612"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n |-- dep_time: integer (nullable = true)\n |-- dep_delay: integer (nullable = true)\n |-- arr_time: integer (nullable = true)\n |-- arr_delay: double (nullable = true)\n |-- carrier: string (nullable = true)\n |-- tailnum: string (nullable = true)\n |-- flight: integer (nullable = true)\n |-- origin: string (nullable = true)\n |-- dest: string (nullable = true)\n |-- air_time: integer (nullable = true)\n |-- distance: integer (nullable = true)\n |-- hour: integer (nullable = true)\n |-- minute: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- year: integer (nullable = true)\n-- month: integer (nullable = true)\n-- day: integer (nullable = true)\n-- dep_time: integer (nullable = true)\n-- dep_delay: integer (nullable = true)\n-- arr_time: integer (nullable = true)\n-- arr_delay: double (nullable = true)\n-- carrier: string (nullable = true)\n-- tailnum: string (nullable = true)\n-- flight: integer (nullable = true)\n-- origin: string (nullable = true)\n-- dest: string (nullable = true)\n-- air_time: integer (nullable = true)\n-- distance: integer (nullable = true)\n-- hour: integer (nullable = true)\n-- minute: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Vamos a volver a mostrar las 5 primeras filas del DataFrame limpio. Aparentemente son iguales a las que ya teníamos, pero ahora\nSpark sí está tratando como enteros las columnas que deberían serlo, y si queremos podemos hacer operaciones aritméticas\ncon ellas."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c7b3811-b614-4f05-9213-4ae4c461071c"}}},{"cell_type":"code","source":["flightsConvertido.show(5)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc609606-50f6-4b82-858b-97e5dc019899"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n|2014|    1|  1|       1|       96|     235|     70.0|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n|2014|    1|  1|       4|       -6|     738|    -23.0|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n|2014|    1|  1|       8|       13|     548|     -4.0|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n|2014|    1|  1|      28|       -2|     800|    -23.0|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n|2014|    1|  1|      34|       44|     325|     43.0|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\nonly showing top 5 rows\n\nOut[75]: [&#39;year&#39;,\n &#39;month&#39;,\n &#39;day&#39;,\n &#39;dep_time&#39;,\n &#39;dep_delay&#39;,\n &#39;arr_time&#39;,\n &#39;arr_delay&#39;,\n &#39;carrier&#39;,\n &#39;tailnum&#39;,\n &#39;flight&#39;,\n &#39;origin&#39;,\n &#39;dest&#39;,\n &#39;air_time&#39;,\n &#39;distance&#39;,\n &#39;hour&#39;,\n &#39;minute&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\nyear|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n2014|    1|  1|       1|       96|     235|     70.0|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n2014|    1|  1|       4|       -6|     738|    -23.0|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n2014|    1|  1|       8|       13|     548|     -4.0|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n2014|    1|  1|      28|       -2|     800|    -23.0|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n2014|    1|  1|      34|       44|     325|     43.0|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\nonly showing top 5 rows\n\nOut[75]: [&#39;year&#39;,\n &#39;month&#39;,\n &#39;day&#39;,\n &#39;dep_time&#39;,\n &#39;dep_delay&#39;,\n &#39;arr_time&#39;,\n &#39;arr_delay&#39;,\n &#39;carrier&#39;,\n &#39;tailnum&#39;,\n &#39;flight&#39;,\n &#39;origin&#39;,\n &#39;dest&#39;,\n &#39;air_time&#39;,\n &#39;distance&#39;,\n &#39;hour&#39;,\n &#39;minute&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Ejercicio 1\n\nPartiendo del DataFrame `flightsConvertido` que ya tiene los tipos correctos en las columnas, se pide: \n\n* Crear un nuevo DataFrame llamado `aeropuertosOrigenDF` que tenga una columna `origin` y que tenga tantas filas como aeropuertos distintos de *origen* existan. ¿Cuántas filas tiene? Almacenar dicho recuento en la variable entera `n_origen`.\n* Crear un nuevo DataFrame llamado `rutasDistintasDF` que tenga dos columnas `origin`, `dest` y que tenga tantas filas como rutas diferentes existan (es decir, como combinaciones distintas haya entre un origen y un destino). Una vez creado, contar cuántas combinaciones hay, almacenando dicho recuento en la variable entera `n_rutas`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8ef56d9-1d7a-481a-8aea-1cea46b6a6b0"}}},{"cell_type":"code","source":["# Reemplaza None por el código necesario para calcular sus valores correctos\naeropuertosOrigenDF = flightsConvertido.select(\"origin\").distinct()\nn_origen = aeropuertosOrigenDF.count()\nrutasDistintasDF = flightsConvertido.select(\"origin\", \"dest\").distinct()\nn_rutas = rutasDistintasDF.count()\n\n# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9e580b4-e638-4398-9512-815130ed9895"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[20]: 115</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[20]: 115</div>"]}}],"execution_count":0},{"cell_type":"code","source":["assert(n_origen == 2)\nassert(n_rutas == 115)\nassert(aeropuertosOrigenDF.count() == n_origen)\nassert(rutasDistintasDF.count() == n_rutas)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecbe1b60-8a34-43bd-9e64-e8ee17b00bb7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Ejercicio 2\n\n* Partiendo de nuevo de `flightsConvertido`, se pide calcular, *sólo para los vuelos que llegan con* ***retraso positivo***, el retraso medio a la llegada de dichos vuelos, para cada aeropuerto de destino. La nueva columna con el retraso medio a la llegada debe llamarse `retraso_medio`. El DF resultante debe estar **ordenado de mayor a menor retraso medio**. El código que calcule esto debería ir encapsulado en una función de Python llamada `retrasoMedio` que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el cálculo descrito anteriormente.\n\n* Una vez hecha la función, invocarla pasándole como argumento `flightsConvertido` y almacenar el resultado devuelto en la variable `retrasoMedioDF`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c9ad34c-5bd7-4bdf-a0bf-ccf921e9fd41"}}},{"cell_type":"code","source":["def retrasoMedio(df):  # El argumento que recibe la función es un DataFrame de Spark\n    import pyspark.sql.functions#Introducido aquí para evitar errores en las correciones\n    from pyspark.sql.functions import desc\n    # Escribe el código de tu función\n    data = df.filter(df.arr_delay > 0).groupBy(df.dest).agg({\"arr_delay\": \"avg\"}).withColumnRenamed(\"AVG(arr_delay)\", \"retraso_medio\").sort(desc(\"avg(arr_delay)\")) #.orderBy(\"avg(arr_delay)\").desc()\n\n    return data  # Reemplaza None por la variable que quieras devolver\n\nretrasoMedioDF = retrasoMedio(flightsConvertido)  # Reemplazar None por el código necesario\n\n# YOUR CODE HERE\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e701df0f-8c89-4238-9db4-a7d4e921f745"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+------------------+\n|dest|     retraso_medio|\n+----+------------------+\n| BOI|             64.75|\n| HDN|              46.8|\n| SFO|41.193768844221104|\n| CLE| 35.74193548387097|\n| SBA|35.391752577319586|\n| COS| 35.05607476635514|\n| BWI|34.585798816568044|\n| EWR| 33.52972258916777|\n| DFW| 33.27519181585678|\n| MIA| 32.66187050359712|\n| ORD| 32.47909024211299|\n| BNA| 31.94871794871795|\n| JFK|31.255884586180713|\n| JAC|             30.25|\n| PHL|29.245989304812834|\n| OGG|27.511111111111113|\n| IAD|27.430875576036865|\n| HOU| 27.33009708737864|\n| LGB| 27.07634730538922|\n| FAT|26.852589641434264|\n+----+------------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------------------+\ndest|     retraso_medio|\n+----+------------------+\n BOI|             64.75|\n HDN|              46.8|\n SFO|41.193768844221104|\n CLE| 35.74193548387097|\n SBA|35.391752577319586|\n COS| 35.05607476635514|\n BWI|34.585798816568044|\n EWR| 33.52972258916777|\n DFW| 33.27519181585678|\n MIA| 32.66187050359712|\n ORD| 32.47909024211299|\n BNA| 31.94871794871795|\n JFK|31.255884586180713|\n JAC|             30.25|\n PHL|29.245989304812834|\n OGG|27.511111111111113|\n IAD|27.430875576036865|\n HOU| 27.33009708737864|\n LGB| 27.07634730538922|\n FAT|26.852589641434264|\n+----+------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["lista = retrasoMedio(flightsConvertido).take(3)\nassert((lista[0].retraso_medio == 64.75) & (lista[0].dest == \"BOI\"))\nassert((lista[1].retraso_medio == 46.8) & (lista[1].dest == \"HDN\"))\nassert((round(lista[2].retraso_medio, 2) == 41.19) & (lista[2].dest == \"SFO\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdc0df59-0470-42bd-a653-3acea241df6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Ahora invocamos a nuestra función `retrasoMedio` pasándole como argumento `flightsConvertido`. ¿Cuáles son los tres aeropuertos con mayor retraso medio? ¿Cuáles son sus retrasos medios en minutos?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c8e8f8f-42d5-41e4-91bd-7c9ccd8487ec"}}},{"cell_type":"code","source":["# ESCRIBE AQUÍ TU CÓDIGO PARA MOSTRAR EL CONTENIDO DE retrasoMedioDF\nretrasoMedioDF.show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0635cd8-404d-4ff2-b3d4-a4b93e399163"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+------------------+\n|dest|     retraso_medio|\n+----+------------------+\n| BOI|             64.75|\n| HDN|              46.8|\n| SFO|41.193768844221104|\n+----+------------------+\nonly showing top 3 rows\n\nOut[74]: [&#39;dest&#39;, &#39;retraso_medio&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------------------+\ndest|     retraso_medio|\n+----+------------------+\n BOI|             64.75|\n HDN|              46.8|\n SFO|41.193768844221104|\n+----+------------------+\nonly showing top 3 rows\n\nOut[74]: [&#39;dest&#39;, &#39;retraso_medio&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Ejercicio 3\n\nAjustar un modelo de DecisionTree de Spark para predecir si un vuelo vendrá o no con retraso (problema de clasificación binaria), utilizando como variables predictoras el mes, el día del mes, la hora de partida `dep_time`, la hora de llegada `arr_time`, el tipo de avión (`carrier`), la distancia y el tiempo que permanece en el aire. Para ello, sigue los siguientes pasos."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9323230-fa28-44d9-9d59-79da3295acab"}}},{"cell_type":"markdown","source":["Notemos que en estos datos hay variables numéricas y variables categóricas que ahora mismo están tipadas como numéricas, como por ejemplo el mes del año (`month`), que es en realidad categórica. Debemos indicar a Spark cuáles son categóricas e indexarlas. Para ello se pide: \n\n* Crear un `StringIndexer` llamado `indexerMonth` y otro llamado `indexerCarrier` sobre las variables categóricas `month` y `carrier` (tipo de avión). El nombre de las columnas indexadas que se crearán debe ser, respectivamente, `monthIndexed` y `carrierIndexed`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0709c2b9-8bea-44da-ba58-73ee69aacf46"}}},{"cell_type":"code","source":["# Incluye aquí los imports que necesites\nfrom pyspark.ml.feature import StringIndexer\n\nindexerMonth = StringIndexer(inputCol=\"month\", outputCol=\"monthIndexed\")\nindexerCarrier = StringIndexer(inputCol=\"carrier\", outputCol=\"carrierIndexed\")\n\n# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce9f5509-bb50-48e3-bb5d-95b09653abd0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["assert(isinstance(indexerMonth, StringIndexer))\nassert(isinstance(indexerCarrier, StringIndexer))\nassert(indexerMonth.getInputCol() == \"month\")\nassert(indexerMonth.getOutputCol() == \"monthIndexed\")\nassert(indexerCarrier.getInputCol() == \"carrier\")\nassert(indexerCarrier.getOutputCol() == \"carrierIndexed\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f838edcd-b2ff-4587-ac9b-fb3828fde902"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Recordemos también que Spark requiere que todas las variables estén en una única columna de tipo vector, por lo que después de indexar estas dos variables, tendremos que fusionar en una columna de tipo vector todas ellas, utilizando un `VectorAssembler`. Se pide:\n\n* Crear en una variable llamada `vectorAssembler` un `VectorAssembler` que reciba como entrada una lista de todas las variables de entrada (y que no debe incluir `arr_delay`) que serán las que formarán parte del modelo. Crear primero esta lista de variables (lista de strings) en la variable `columnas_ensamblar` y pasar dicha variable como argumento al crear el `VectorAssembler`. Como es lógico, en el caso de las columnas `month` y `carrier`, no usaremos las variables originales sino las indexadas en el apartado anterior. La columna de tipo vector creada con las características ensambladas debe llamarse `features`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ee37c6e-652c-44bd-a1f2-c6d184789750"}}},{"cell_type":"code","source":["# Incluye aquí los imports que necesites\nfrom pyspark.ml.feature import VectorAssembler\n\n#columnas_ensamblar = ['year','month','day','dep_time','dep_delay','arr_time','arr_delay','carrier','tailnum','flight','origin','dest','air_time','distance','hour','minute']\n#columnas_ensamblar = [indexerMonth,'day','dep_time','arr_time',indexerCarrier,'air_time','distance']\ncolumnas_ensamblar = [indexerMonth.getOutputCol(),'day','dep_time','arr_time',indexerCarrier.getOutputCol(),'air_time','distance']\nvectorAssembler = VectorAssembler(inputCols=columnas_ensamblar, outputCol=\"features\")\n\n\n# YOUR CODE HERE\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79563dc4-ec79-4c21-aae2-f10a6ab3cfd1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["assert(isinstance(vectorAssembler, VectorAssembler))\nassert(vectorAssembler.getOutputCol() == \"features\")\ninput_cols = vectorAssembler.getInputCols()\nassert(len(input_cols) == 7)\nassert(\"arr_delay\" not in input_cols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f99e4e7f-f48b-45db-ac7f-4d0853b5ab03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Finalmente, vemos que la columna `arr_delay` es continua, y no binaria como requiere un problema de clasificación con dos clases. Vamos a convertirla en binaria. Para ello se pide:\n\n* Utilizar un binarizador de Spark, fijando a 15 el umbral, y guardarlo en la variable `delayBinarizer`. Consideramos retrasado un vuelo que ha llegado con más de 15 minutos de retraso, y no retrasado en caso contrario. La nueva columna creada con la variable binaria debe llamarse `arr_delay_binary` y debe ser interpretada como la columna target para nuestro algoritmo. Por ese motivo, esta columna **no** se incluyó en el apartado anterior entre las columnas que se ensamblan para formar las features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cdcb6ba-991b-4529-a08c-dccc229a4b89"}}},{"cell_type":"code","source":["# Incluye aquí los imports que necesites y que no hayas incluido ya en alguna celda anterior\nfrom pyspark.ml.feature import Binarizer\n\ndelayBinarizer = Binarizer(threshold=15.0, inputCol=\"arr_delay\", outputCol=\"arr_delay_binary\")\n\n# YOUR CODE HERE\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e7d3584-4c90-4461-9700-8839406ec0ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["assert(isinstance(delayBinarizer, Binarizer))\nassert(delayBinarizer.getThreshold() == 15)\nassert(delayBinarizer.getInputCol() == \"arr_delay\")\nassert(delayBinarizer.getOutputCol() == \"arr_delay_binary\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97d6cffe-02b1-4bd3-9f9e-a43f07e7e828"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Por último, crearemos el modelo de clasificación.\n\n* Crear en una variable `decisionTree` un árbol de clasificación de Spark (`DecisionTreeClassifier` del paquete `pyspark.ml.classification`)\n* Indicar como columna de entrada la nueva columna creada por el `VectorAssembler` creado en un apartado anterior.\n* Indicar como columna objetivo (target) la nueva columna creada por el `Binarizer` del apartado anterior."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d57e28bc-a955-4a4c-969c-64eebcb4bff9"}}},{"cell_type":"code","source":["# Incluye aquí los imports que necesites y que no hayas incluido ya en alguna celda anterior\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\ndecisionTree = DecisionTreeClassifier(featuresCol=vectorAssembler.getOutputCol(), labelCol=delayBinarizer.getOutputCol())\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4246137-1dc3-4b1e-adcb-86f8f3234bbf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["assert(isinstance(decisionTree, DecisionTreeClassifier))\nassert(decisionTree.getFeaturesCol() == \"features\")\nassert(decisionTree.getLabelCol() == \"arr_delay_binary\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2814896-01ef-4789-adbb-7d93e2708452"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Ahora vamos a encapsular todas las fases en un sólo pipeline y procederemos a entrenarlo. Se pide:\n\n* Crear en una variable llamada `pipeline` un objeto `Pipeline` de Spark con las etapas anteriores en el orden adecuado para poder entrenar un modelo. \n\n* Entrenarlo invocando sobre ella al método `fit` y guardar el pipeline entrenado devuelto por dicho método en una variable llamada `pipelineModel`. \n\n* Aplicar el pipeline entrenado para transformar (predecir) el DataFrame `flightsConvertido`, guardando las predicciones devueltas en la variable `flightsPredictions` que será un DataFrame. Nótese que estamos prediciendo los propios datos de entrenamiento y que, por simplicidad, no habíamos hecho (aunque habría sido lo correcto) ninguna división de nuestros datos originales en subconjuntos distintos de entrenamiento y test antes de entrenar."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"add4c427-46fb-40d7-a39e-7a75c6ba6671"}}},{"cell_type":"code","source":["# Incluye aquí los imports que necesites y que no hayas incluido ya en alguna celda anterior\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = [indexerMonth, indexerCarrier, vectorAssembler, delayBinarizer, decisionTree])\npipelineModel = pipeline.fit(flightsConvertido)\nflightsPredictions = pipelineModel.transform(flightsConvertido)\n\n# YOUR CODE HERE\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6e27a04-3255-429e-bc04-0f8d953f713b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml import PipelineModel\nassert(isinstance(pipeline, Pipeline))\nassert(len(pipeline.getStages()) == 5)\nassert(isinstance(pipelineModel, PipelineModel))\nassert(\"probability\" in flightsPredictions.columns)\nassert(\"prediction\" in flightsPredictions.columns)\nassert(\"rawPrediction\" in flightsPredictions.columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c5c847e-3648-4717-84ff-edb7002f3026"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Vamos a mostrar la matriz de confusión (este apartado no es evaluable). Agrupamos por la variable que tiene la clase verdadera y la que tiene la clase predicha, para ver en cuántos casos coinciden y en cuántos difieren."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"318c3e39-9a04-47be-9211-0f31abafdca4"}}},{"cell_type":"code","source":["flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"014ab36e-69b7-46ca-9ead-88624f02f80a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------------+----------+------+\n|arr_delay_binary|prediction| count|\n+----------------+----------+------+\n|             1.0|       1.0|   469|\n|             0.0|       1.0|    50|\n|             1.0|       0.0| 23780|\n|             0.0|       0.0|136449|\n+----------------+----------+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+----------+------+\narr_delay_binary|prediction| count|\n+----------------+----------+------+\n             1.0|       1.0|   469|\n             0.0|       1.0|    50|\n             1.0|       0.0| 23780|\n             0.0|       0.0|136449|\n+----------------+----------+------+\n\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"actividad_1","dashboards":[],"language":"python","widgets":{},"notebookOrigID":916427626918979}},"nbformat":4,"nbformat_minor":0}
