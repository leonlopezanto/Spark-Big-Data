{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "9009124b4c18f0eda044abc5e2ff141f", "grade": false, "grade_id": "cell-20cd3bb8f5a12f07", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "# Actividad 2: Structured Streaming y Kafka"}, {"cell_type": "markdown", "metadata": {}, "source": "## Recuerda borrar siempre las l\u00edneas que dicen `raise NotImplementedError`"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "c1fed64c5c94f8b4c27326b8b48ec13f", "grade": false, "grade_id": "cell-2fbae7fd82212064", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Punto de partida (final de la actividad 1): funci\u00f3n `retrasoMedio` \n***Para los vuelos que llegan con retraso positivo, calcular para cada aeropuerto de llegada el retraso medio.***\n\nRecordatorio: *El c\u00f3digo que calcule esto deber\u00eda ir encapsulado en una funci\u00f3n de Python que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el c\u00e1lculo del retraso medio por aeropuerto, ordenado de mayor a menor retraso medio. La columna creada con el retraso medio debe llamarse `retraso_medio`.*\n\n**Copia en la siguiente celda el c\u00f3digo de tu funci\u00f3n retrasoMedio que has completado en la actividad 1**. El DataFrame devuelto por la funci\u00f3n deber\u00eda tener solamente dos columnas: `dest` y `retraso_medio`."}, {"cell_type": "code", "execution_count": 166, "metadata": {}, "outputs": [], "source": "import pyspark.sql.functions #Introducido aqu\u00ed para evitar errores en las correciones\nfrom pyspark.sql.functions import *\n\ndef retrasoMedio(df):\n\n    # Escribe el c\u00f3digo de tu funci\u00f3n\n    data = df.filter(df.arr_delay > 0)\\\n    .groupBy(df.dest).agg({\"arr_delay\": \"avg\"})\\\n    .withColumnRenamed(\"AVG(arr_delay)\", \"retraso_medio\")\\\n    .sort(desc(\"avg(arr_delay)\")) #.orderBy(\"avg(arr_delay)\").desc()\n    \n    return data"}, {"cell_type": "markdown", "metadata": {}, "source": "### Ejercicio 1\n\nUtilizaremos Kafka para actualizar en tiempo real el resultado calculado en el apartado anterior. \n\nPara simplificar, asumimos que los mensajes le\u00eddos de Kafka tiene solamente dos campos que son los \u00fanicos necesarios para llevar a cabo la operaci\u00f3n anterior: dest y arr_delay. La idea ser\u00e1 crear un Streaming DataFrame para leer de Kafka, y despu\u00e9s invocar a nuestra funci\u00f3n retrasoMedio pas\u00e1ndolo como argumento. Vamos a leer del topic `retrasos` por lo que debes indicar esta opci\u00f3n a continuaci\u00f3n.\n\nSe pide crear, en la variable `retrasosStreamingDF`, un Streaming DataFrame leyendo de Apache Kafka, configurando las siguientes opciones:\n  * Usar la variable `readStream` (en lugar de `read` como solemos hacer) interna de la SparkSession `spark`\n  * Indicar que el formato es `\"kafka\"` con `.format(\"kafka\")`\n  * Indicar cu\u00e1les son los brokers de Kafka de los que vamos a leer y el puerto al que queremos conectarnos para leer (9092 es el que usa Kafka por defecto), con `.option(\"kafka.bootstrap.servers\", \"<nombre_cluster>-w-0:9092,<nombre_cluster>-w-1:9092\")`. De esa manera podremos leer el mensaje si el productor de Kafka lo env\u00eda a cualquiera de los dos brokers existentes, que son los nodos del cluster identificados como `<nombre_cluster>-w-0` y `<nombre_cluster>-w-1`\n  * Indicar que queremos subscribirnos al topic `\"retrasos\"` con `.option(\"subscribe\", \"retrasos\")`.\n  * Finalmente ponemos `load()` para realizar la lectura."}, {"cell_type": "code", "execution_count": 172, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "ca3d9ff34a83745e199a3c4aed9acf07", "grade": false, "grade_id": "read-stream", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# Reemplaza por el c\u00f3digo correcto siguiendo las indicaciones anteriores\n\nretrasosStreamingDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"cluster-a4b5-w-0:9092, cluster-a4b5-w-1:9092\").option(\"subscribe\", \"retrasos\").load()\\\n    "}, {"cell_type": "code", "execution_count": 168, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "eb5c34086b3c3fb29a4c46865396bcb1", "grade": true, "grade_id": "read-stream-tests", "locked": true, "points": 2, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "# Mostramos el esquema de este DataFrame\ntypes = retrasosStreamingDF.dtypes\nassert(retrasosStreamingDF.isStreaming)\nassert((types[0][0] == \"key\")       & (types[0][1] == \"binary\"))\nassert((types[1][0] == \"value\")     & (types[1][1] == \"binary\"))\nassert((types[2][0] == \"topic\")     & (types[2][1] == \"string\"))\nassert((types[3][0] == \"partition\") & (types[3][1] == \"int\"))\nassert((types[4][0] == \"offset\")    & (types[4][1] == \"bigint\"))\nassert((types[5][0] == \"timestamp\") & (types[5][1] == \"timestamp\"))\nassert((types[6][0] == \"timestampType\") & (types[6][1] == \"int\"))"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "699b1979df4eb0241b1b4ba165d6b311", "grade": false, "grade_id": "cell-580bf3caf39b314e", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Muestra por pantalla el esquema del DataFrame resultante de la lectura con `printSchema()`. Ver\u00e1s que todas estas columnas son creadas autom\u00e1ticamente por Spark cuando leemos de Kafka. De ellas, la que nos interesa es `value` que contiene propiamente el mensaje de Kafka, en formato datos binarios. \n\n### Ejercicio 2\n\nTendremos que estructurar estos datos para poder extraer los campos. Para ello sigue los siguientes pasos, ayud\u00e1ndote de la plantilla que hay en la celda siguiente (descom\u00e9ntala y compl\u00e9tala):\n\n* **Selecciona** la columna `value` y convi\u00e9rtela (`.cast`) a `StringType()` utilizando `withColumn` para reemplazar la columna existente `\"value\"` por el objeto Column resultante de la conversi\u00f3n. De esta forma tendremos una columna que contendr\u00e1 en cada **fila** un **fichero JSON completo**, tal como se muestra en cada una de las plantillas anteriores. \n* Para extraer los dos campos de cada uno de los JSON y convertirlos en una columna llamada `parejas`, de tipo `struct` (una estructura formada por dos campos de tipo String e Integer respectivamente), utilizamos la funci\u00f3n `from_json` de Spark, que se aplica a cada elemento (cada fila) de la columna \"value\" y parsea el String seg\u00fan un esquema que le indiquemos, devolviendo una columna de tipo `struct`.\n* La columna `parejas` es de tipo `struct` por lo que puedes acceder a cada uno de sus dos campos (`dest` y `arr_delay`) con el operador `.` (punto). Utilizando `withColumn` dos veces, crea dos columnas llamadas `dest` y `arr_delay` como el resultado de acceder a `parejas.dest` y `parejas.arr_delay` respectivamente."}, {"cell_type": "code", "execution_count": 173, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "91b265facf6e3bded458cc9b6a754b1a", "grade": false, "grade_id": "estructura-json", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\nimport pyspark.sql.functions as F\n\nesquema = StructType([\\\n  StructField(\"dest\", StringType()),\\\n  StructField(\"arr_delay\", DoubleType())\\\n])\n\nparsedDF = retrasosStreamingDF\\\n     .select(\"value\")\\\n     .withColumn(\"value\", F.col(\"value\").cast(StringType()))\\\n     .withColumn(\"parejas\", F.from_json(F.col(\"value\"), esquema))\\\n     .withColumn(\"dest\", F.col(\"parejas.dest\"))\\\n     .withColumn(\"arr_delay\", F.col(\"parejas.arr_delay\"))\n"}, {"cell_type": "code", "execution_count": 174, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "6fc63420d404c6ca1fedec52d4ee3b7a", "grade": true, "grade_id": "estructura-json-tests", "locked": true, "points": 3, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "tipos = parsedDF.dtypes\nassert((\"value\", \"string\") in tipos)\nassert(('parejas', 'struct<dest:string,arr_delay:double>') in tipos)\nassert(('dest', 'string') in tipos)\nassert(('arr_delay', 'double') in tipos)"}, {"cell_type": "markdown", "metadata": {}, "source": "Nuestro DataFrame ya contiene una columna `dest` con el nombre del aeropuerto destino y una columna de n\u00fameros reales `arr_delay` con el retraso. Ya podemos efectuar el mismo tipo de agregaci\u00f3n que estamos haciendo en nuestra funci\u00f3n `retrasoMedio`. Por tanto, invocamos a `retrasoMedio` pasando `parsedDF` como argumento."}, {"cell_type": "code", "execution_count": 202, "metadata": {}, "outputs": [{"ename": "IllegalArgumentException", "evalue": "'Cannot start query with name retrasosAgg as a query with that name is already active'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o750.start.\n: java.lang.IllegalArgumentException: Cannot start query with name retrasosAgg as a query with that name is already active\n\tat org.apache.spark.sql.streaming.StreamingQueryManager$$anonfun$startQuery$1.apply(StreamingQueryManager.scala:338)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager$$anonfun$startQuery$1.apply(StreamingQueryManager.scala:336)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:336)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:267)\n\tat sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)", "\u001b[0;32m<ipython-input-202-1891f7faa570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"retrasosAgg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Cannot start query with name retrasosAgg as a query with that name is already active'"]}], "source": "# Eval\u00faa el siguiente c\u00f3digo pero no lo modifiques\n# Indicamos que este DataFrame se guarde en memoria cuando se va actualizando,\n# y arrancamos la ejecuci\u00f3n en Streaming con la acci\u00f3n start()\n\nretrasoMedioStreamingDF = retrasoMedio(parsedDF)\n\nconsoleOutput = retrasoMedioStreamingDF\\\n                    .writeStream\\\n                    .queryName(\"retrasosAgg\")\\\n                    .outputMode(\"complete\")\\\n                    .format(\"memory\")\\\n                    .start()\n"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "018ce42cfbc94eef069b4a60e5e89090", "grade": false, "grade_id": "cell-b073802f3eec8bb3", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Una vez evaluada la celda anterior, abre el productor de Kafka console entrando por SSH a cualquiera de las m\u00e1quinas (revisa el enunciado de la pr\u00e1ctica para recordarlo), y copia y pega (literalmente) los siguientes 4 mensajes en formato JSON. Como ves,  tienen un campo `dest` y un campo `arr_delay`, simulando la informaci\u00f3n que estar\u00edamos recibiendo en tiempo real de los distintos aeropuertos a medida que los vuelos van aterrizando. \n\nCada vez que pegues un mensaje, ejecuta la consulta `select * from retrasosAgg` a trav\u00e9s del m\u00e9todo `spark.sql(...)` y muestra el DataFrame `agregadosDF` devuelto por dicho m\u00e9todo. Eso har\u00e1 una consulta contra la vista temporal (vol\u00e1til) `retrasosAgg` que se ha creado en el metastore de Hive gracias al `writeStream` del apartado anterior. Ejecuta la celda de `show` tantas veces como sea necesario hasta ver un resultado distinto al que has visto en la ejecuci\u00f3n anterior, para asegurarte de que Spark ya ha le\u00eddo e incorporado el nuevo dato en su c\u00e1lculo de la agregaci\u00f3n y por tanto ha actualizado el resultado.\n\nRecuerda que el m\u00e9todo `.sql(...)` es una transformaci\u00f3n, y por tanto, se re-ejecuta la consulta cada vez que invocas a la acci\u00f3n `show()` sobre el resultado, ya que **no vamos a cachear nada**, precisamente para forzar la reevaluaci\u00f3n de la consulta y poder ver as\u00ed el contenido actualizado de dicha tabla (en memoria) de Hive cada vez que hacemos `show()`.\n\nSe pide: \n* Cada vez que env\u00edes un mensaje y te hayas asegurado de que Spark ha incorporado el dato a su c\u00e1lculo, apunta el resultado de la agregaci\u00f3n (valor de la columna `retraso_medio`) para MAD y GRX en las variables habilitadas para ello\n* No te preocupes por evaluar muchas veces una misma celda, ya que el c\u00e1lculo s\u00f3lo se actualizar\u00e1 una vez. Las siguientes veces que la eval\u00faes seguir\u00e1 mostrando el mismo resultado mientras no env\u00edes otro nuevo mensaje en Kafka.\n\nLos 4 mensajes que hay que introducir sucesivamente en Kafka son:\n\n`\n{\"dest\": \"GRX\", \"arr_delay\": 2.6}\n{\"dest\": \"MAD\", \"arr_delay\": 5.4}\n{\"dest\": \"GRX\", \"arr_delay\": 1.5}\n{\"dest\": \"MAD\", \"arr_delay\": 20.0}\n`"}, {"cell_type": "code", "execution_count": 196, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "aca37e32dce02202f512568edca47c19", "grade": false, "grade_id": "sql-streaming", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "agregadosDF = spark.sql(\"SELECT * FROM retrasosAgg\")   # Modifica esta l\u00ednea para invocar al m\u00e9todo .sql de la spark session\n"}, {"cell_type": "code", "execution_count": 197, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "4c17f79058004e6c56c6cb64795ae6f2", "grade": true, "grade_id": "sql-streaming-test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "columnas = agregadosDF.columns\nassert(len(columnas) == 2)\nassert(\"dest\" in columnas)\nassert(\"retraso_medio\" in columnas)"}, {"cell_type": "code", "execution_count": 199, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "709560e7fbed0009d74b7fec8c90a3c1", "grade": false, "grade_id": "results-1", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+-------------+\n|dest|retraso_medio|\n+----+-------------+\n| MAD|         12.7|\n| GRX|         2.05|\n+----+-------------+\n\n"}], "source": "agregadosDF.show()  # Ejecuta varias esta celda tras enviar el primer mensaje, hasta ver que el DataFrame no es vac\u00edo\nretraso_medio_GRX_primer_mensaje = 2.6  # Apunta este dato (manualmente)\n\n"}, {"cell_type": "code", "execution_count": 190, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "9b8f02459cd58afae6cf1a32ef69f36e", "grade": false, "grade_id": "results-2", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+-------------+\n|dest|retraso_medio|\n+----+-------------+\n| MAD|          5.4|\n| GRX|         2.05|\n+----+-------------+\n\n"}], "source": "# Ejecuta varias veces esta celda tras enviar el segundo mensaje, hasta ver que el DataFrame ha cambiado\nagregadosDF.show()\nretraso_medio_GRX_segundo_mensaje = 2.6  # Apunta este dato (manualmente)\nretraso_medio_MAD_segundo_mensaje = 5.4  # Apunta este dato (manualmente)\n\n"}, {"cell_type": "code", "execution_count": 192, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "8ed1d482d7859eaf314d85f7f056d79e", "grade": false, "grade_id": "results-3", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+-------------+\n|dest|retraso_medio|\n+----+-------------+\n| MAD|          5.4|\n| GRX|         2.05|\n+----+-------------+\n\n"}], "source": "# Ejecuta varias veces esta celda tras enviar el tercer mensaje, hasta ver que el DataFrame ha cambiado\nagregadosDF.show()\nretraso_medio_GRX_tercer_mensaje = 2.05  # Apunta este dato (manualmente)\nretraso_medio_MAD_tercer_mensaje = 5.4  # Apunta este dato (manualmente)\n\n# YOUR CODE HERE\n"}, {"cell_type": "code", "execution_count": 194, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "0e49b6258fd677f636bbc9f01e41a593", "grade": false, "grade_id": "results-4", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+-------------+\n|dest|retraso_medio|\n+----+-------------+\n| MAD|         12.7|\n| GRX|         2.05|\n+----+-------------+\n\n"}], "source": "# Ejecuta varias veces esta celda tras enviar el cuarto mensaje, hasta ver que el DataFrame ha cambiado\nagregadosDF.show()\nretraso_medio_GRX_cuarto_mensaje = 12.7  # Apunta este dato (manualmente)\nretraso_medio_MAD_cuarto_mensaje = 2.05  # Apunta este dato (manualmente)\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "065e98ac2bd3061d7706cd282041e722", "grade": true, "grade_id": "results-tests", "locked": true, "points": 4, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.12"}}, "nbformat": 4, "nbformat_minor": 2}